{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8b9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f13686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech-to-Text Evaluation\n",
    "jiwer = \"^3.3.1\"              # Latest stable version\n",
    "rapidfuzz = \"^3.10.1\"         # Latest version with performance improvements\n",
    "\n",
    "# Diarization Evaluation  \n",
    "numpy = \"^2.2.0\"              # Latest stable version\n",
    "pyannote-audio = \"^3.3.2\"     # Includes pyannote.metrics\n",
    "pyannote-core = \"^5.0.0\"      # Core annotation utilities\n",
    "\n",
    "# Optional Configuration\n",
    "pyyaml = \"^6.0.2\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a5acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'Geethuzzz/common_voice_17_0_arabic_New_cleaned' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to: /workspaces/oryx-cap-upskilling/eval/test notebooks/../datasets/stt/arabic_common_voice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28369/28369 [00:04<00:00, 6012.81 examples/s]\n",
      "Generating validation split:   0%|          | 0/10470 [00:00<?, ? examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10470/10470 [00:01<00:00, 5571.32 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10470/10470 [00:01<00:00, 5571.32 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10480/10480 [00:01<00:00, 5377.80 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10480/10480 [00:01<00:00, 5377.80 examples/s]\n",
      "Generating other split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41628/41628 [00:07<00:00, 5244.69 examples/s]\n",
      "Generating other split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41628/41628 [00:07<00:00, 5244.69 examples/s]\n",
      "Generating invalidated split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15108/15108 [00:04<00:00, 3634.48 examples/s]\n",
      "Generating validated split:   0%|          | 0/78156 [00:00<?, ? examples/s]\n",
      "Generating validated split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78156/78156 [00:13<00:00, 5674.96 examples/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n",
      "Dataset splits: ['train', 'validation', 'test', 'other', 'invalidated', 'validated']\n",
      "train: 28369 samples\n",
      "validation: 10470 samples\n",
      "test: 10480 samples\n",
      "other: 41628 samples\n",
      "invalidated: 15108 samples\n",
      "validated: 78156 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create local datasets directory\n",
    "local_data_dir = Path(\"../datasets/stt/arabic_common_voice\")\n",
    "local_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Downloading dataset to: {local_data_dir.absolute()}\")\n",
    "\n",
    "# Download and cache the dataset locally\n",
    "# Login using `huggingface-cli login` if you haven't already\n",
    "try:\n",
    "    ds = load_dataset(\n",
    "        \"Geethuzzz/common_voice_17_0_arabic_New_cleaned\",\n",
    "        cache_dir=str(local_data_dir / \"cache\"),\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "    print(f\"Dataset splits: {list(ds.keys())}\")\n",
    "    \n",
    "    # Show basic info about the dataset\n",
    "    for split_name, split_data in ds.items():\n",
    "        print(f\"{split_name}: {len(split_data)} samples\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    print(\"Make sure you're logged in with: huggingface-cli login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae38411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset splits to local files...\n",
      "\n",
      "Processing train split:\n",
      "Columns: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant']\n",
      "Sample count: 28369\n",
      "Saved to: ../datasets/stt/arabic_common_voice/train/train.parquet\n",
      "Sample saved to: ../datasets/stt/arabic_common_voice/train/train_sample.json\n",
      "Audio column present with 28369 non-null entries\n",
      "Text columns found: ['sentence']\n",
      "Sample sentence: ÙˆÙ…Ø§ ÙƒØ§Ù† Ø±Ø¨Ùƒ Ù„ÙŠÙ‡Ù„Ùƒ Ø§Ù„Ù‚Ø±Ù‰ Ø¨Ø¸Ù„Ù… ÙˆØ£Ù‡Ù„Ù‡Ø§ Ù…ØµÙ„Ø­ÙˆÙ†\n",
      "\n",
      "Processing validation split:\n",
      "Columns: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant']\n",
      "Sample count: 10470\n",
      "Saved to: ../datasets/stt/arabic_common_voice/validation/validation.parquet\n",
      "Sample saved to: ../datasets/stt/arabic_common_voice/validation/validation_sample.json\n",
      "Audio column present with 10470 non-null entries\n",
      "Text columns found: ['sentence']\n",
      "Sample sentence: Ù‡Ùˆ ÙŠÙ‚Ø±Ø£ ÙƒØªØ§Ø¨Ø§Ù‹.\n",
      "\n",
      "Processing test split:\n",
      "Columns: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant']\n",
      "Sample count: 10480\n",
      "Saved to: ../datasets/stt/arabic_common_voice/test/test.parquet\n",
      "Sample saved to: ../datasets/stt/arabic_common_voice/test/test_sample.json\n",
      "Audio column present with 10480 non-null entries\n",
      "Text columns found: ['sentence']\n",
      "Sample sentence: Ù‡Ùˆ ÙŠØ­Ø¨Ù‘Ù‡Ø§ Ùˆ Ù‡ÙŠ ØªØ­Ø¨Ù‘Ù‡ Ø£ÙŠØ¶Ø§.\n",
      "\n",
      "Processing other split:\n",
      "Columns: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant']\n",
      "Sample count: 41628\n",
      "Saved to: ../datasets/stt/arabic_common_voice/other/other.parquet\n",
      "Sample saved to: ../datasets/stt/arabic_common_voice/other/other_sample.json\n",
      "Audio column present with 41628 non-null entries\n",
      "Text columns found: ['sentence']\n",
      "Sample sentence: ØªØªØ£Ù„Ù Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ù…Ù† Ø«Ù„Ø§Ø«Ø© Ø£Ø¬Ø²Ø§Ø¡.\n",
      "\n",
      "Processing invalidated split:\n",
      "Columns: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant']\n",
      "Sample count: 15108\n",
      "Saved to: ../datasets/stt/arabic_common_voice/invalidated/invalidated.parquet\n",
      "Sample saved to: ../datasets/stt/arabic_common_voice/invalidated/invalidated_sample.json\n",
      "Audio column present with 15108 non-null entries\n",
      "Text columns found: ['sentence']\n",
      "Sample sentence: .ÙƒØ«ÙŠØ±Ø§Ù‹ Ù…Ø§ Ø£Ù„Ø¹Ø¨ Ù…Ø¹Ù‡ Ø§Ù„ØªÙ†Ø³\n",
      "\n",
      "Processing validated split:\n",
      "Columns: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant']\n",
      "Sample count: 78156\n",
      "Saved to: ../datasets/stt/arabic_common_voice/validated/validated.parquet\n",
      "Sample saved to: ../datasets/stt/arabic_common_voice/validated/validated_sample.json\n",
      "Audio column present with 78156 non-null entries\n",
      "Text columns found: ['sentence']\n",
      "Sample sentence: Ù‡Ùˆ ÙŠØ­Ø¨Ù‘Ù‡Ø§ Ùˆ Ù‡ÙŠ ØªØ­Ø¨Ù‘Ù‡ Ø£ÙŠØ¶Ø§.\n",
      "\n",
      "Dataset successfully saved to: /workspaces/oryx-cap-upskilling/eval/test notebooks/../datasets/stt/arabic_common_voice\n"
     ]
    }
   ],
   "source": [
    "# Save dataset splits to local files for evaluation framework\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "if 'ds' in locals() and ds is not None:\n",
    "    print(\"Saving dataset splits to local files...\")\n",
    "    \n",
    "    for split_name, split_data in ds.items():\n",
    "        # Create directory for this split\n",
    "        split_dir = local_data_dir / split_name\n",
    "        split_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Convert to pandas for easier handling\n",
    "        df = split_data.to_pandas()\n",
    "        \n",
    "        print(f\"\\nProcessing {split_name} split:\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(f\"Sample count: {len(df)}\")\n",
    "        \n",
    "        # Save as parquet for efficient storage\n",
    "        parquet_file = split_dir / f\"{split_name}.parquet\"\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "        print(f\"Saved to: {parquet_file}\")\n",
    "        \n",
    "        # Also save first 100 samples as JSON for quick inspection (excluding 'audio')\n",
    "        sample_file = split_dir / f\"{split_name}_sample.json\"\n",
    "        sample_data = df.head(100).drop(columns=['audio'], errors='ignore').to_dict('records')\n",
    "        with open(sample_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Sample saved to: {sample_file}\")\n",
    "        \n",
    "        # If there's audio data, show some info\n",
    "        if 'audio' in df.columns:\n",
    "            print(f\"Audio column present with {df['audio'].notna().sum()} non-null entries\")\n",
    "        \n",
    "        # Show first few text samples if available\n",
    "        text_columns = [col for col in df.columns if 'text' in col.lower() or 'sentence' in col.lower()]\n",
    "        if text_columns:\n",
    "            print(f\"Text columns found: {text_columns}\")\n",
    "            for col in text_columns[:1]:  # Show first text column\n",
    "                print(f\"Sample {col}: {df[col].iloc[0] if len(df) > 0 else 'N/A'}\")\n",
    "    \n",
    "    print(f\"\\nDataset successfully saved to: {local_data_dir.absolute()}\")\n",
    "else:\n",
    "    print(\"Dataset not loaded. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5eeb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/oryx-cap-upskilling/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'openslr/librispeech_asr' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'openslr/librispeech_asr' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LibriSpeech dataset to: /workspaces/oryx-cap-upskilling/eval/test notebooks/../datasets/stt/librispeech_clean\n",
      "LibriSpeech dataset downloaded successfully!\n",
      "Dataset splits: ['test', 'train.100', 'train.360', 'validation']\n",
      "test: 2620 samples\n",
      "train.100: 28539 samples\n",
      "train.360: 104014 samples\n",
      "validation: 2703 samples\n",
      "LibriSpeech dataset downloaded successfully!\n",
      "Dataset splits: ['test', 'train.100', 'train.360', 'validation']\n",
      "test: 2620 samples\n",
      "train.100: 28539 samples\n",
      "train.360: 104014 samples\n",
      "validation: 2703 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create local datasets directory for LibriSpeech\n",
    "librispeech_data_dir = Path(\"../datasets/stt/librispeech_clean\")\n",
    "librispeech_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Downloading LibriSpeech dataset to: {librispeech_data_dir.absolute()}\")\n",
    "\n",
    "# Download and cache the LibriSpeech dataset locally\n",
    "try:\n",
    "    librispeech_ds = load_dataset(\n",
    "        \"openslr/librispeech_asr\", \n",
    "        \"clean\",\n",
    "        cache_dir=str(librispeech_data_dir / \"cache\"),\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"LibriSpeech dataset downloaded successfully!\")\n",
    "    print(f\"Dataset splits: {list(librispeech_ds.keys())}\")\n",
    "    \n",
    "    # Show basic info about the dataset\n",
    "    for split_name, split_data in librispeech_ds.items():\n",
    "        print(f\"{split_name}: {len(split_data)} samples\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading LibriSpeech dataset: {e}\")\n",
    "    print(\"Make sure you have internet connection and sufficient disk space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LibriSpeech dataset splits to local files...\n",
      "\n",
      "Processing test split:\n",
      "Columns: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id']\n",
      "Sample count: 2620\n",
      "\n",
      "Processing test split:\n",
      "Columns: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id']\n",
      "Sample count: 2620\n",
      "Saved to: ../datasets/stt/librispeech_clean/test/test.parquet\n",
      "Sample saved to: ../datasets/stt/librispeech_clean/test/test_sample.json\n",
      "Audio column present with 2620 non-null entries\n",
      "Text columns found: ['text']\n",
      "Sample text: CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS\n",
      "Unique speakers: 40\n",
      "Saved to: ../datasets/stt/librispeech_clean/test/test.parquet\n",
      "Sample saved to: ../datasets/stt/librispeech_clean/test/test_sample.json\n",
      "Audio column present with 2620 non-null entries\n",
      "Text columns found: ['text']\n",
      "Sample text: CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS\n",
      "Unique speakers: 40\n",
      "\n",
      "Processing train.100 split:\n",
      "Columns: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id']\n",
      "Sample count: 28539\n",
      "\n",
      "Processing train.100 split:\n",
      "Columns: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id']\n",
      "Sample count: 28539\n",
      "Saved to: ../datasets/stt/librispeech_clean/train.100/train.100.parquet\n",
      "Saved to: ../datasets/stt/librispeech_clean/train.100/train.100.parquet\n",
      "Sample saved to: ../datasets/stt/librispeech_clean/train.100/train.100_sample.json\n",
      "Audio column present with 28539 non-null entries\n",
      "Text columns found: ['text']\n",
      "Sample text: CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED\n",
      "Unique speakers: 251\n",
      "Sample saved to: ../datasets/stt/librispeech_clean/train.100/train.100_sample.json\n",
      "Audio column present with 28539 non-null entries\n",
      "Text columns found: ['text']\n",
      "Sample text: CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED\n",
      "Unique speakers: 251\n"
     ]
    }
   ],
   "source": [
    "# Save LibriSpeech dataset splits to local files for evaluation framework\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "if 'librispeech_ds' in locals() and librispeech_ds is not None:\n",
    "    print(\"Saving LibriSpeech dataset splits to local files...\")\n",
    "    \n",
    "    for split_name, split_data in librispeech_ds.items():\n",
    "        # Create directory for this split\n",
    "        split_dir = librispeech_data_dir / split_name\n",
    "        split_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Convert to pandas for easier handling\n",
    "        df = split_data.to_pandas()\n",
    "        \n",
    "        print(f\"\\nProcessing {split_name} split:\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(f\"Sample count: {len(df)}\")\n",
    "        \n",
    "        # Save as parquet for efficient storage\n",
    "        parquet_file = split_dir / f\"{split_name}.parquet\"\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "        print(f\"Saved to: {parquet_file}\")\n",
    "        \n",
    "        # Also save first 100 samples as JSON for quick inspection (excluding 'audio')\n",
    "        sample_file = split_dir / f\"{split_name}_sample.json\"\n",
    "        sample_data = df.head(100).drop(columns=['audio'], errors='ignore').to_dict('records')\n",
    "        with open(sample_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Sample saved to: {sample_file}\")\n",
    "        \n",
    "        # If there's audio data, show some info\n",
    "        if 'audio' in df.columns:\n",
    "            print(f\"Audio column present with {df['audio'].notna().sum()} non-null entries\")\n",
    "            # Show audio sample rate info if available\n",
    "            if len(df) > 0 and df['audio'].iloc[0] is not None:\n",
    "                sample_audio = df['audio'].iloc[0]\n",
    "                if isinstance(sample_audio, dict) and 'sampling_rate' in sample_audio:\n",
    "                    print(f\"Sample rate: {sample_audio['sampling_rate']} Hz\")\n",
    "        \n",
    "        # Show first few text samples if available\n",
    "        text_columns = [col for col in df.columns if 'text' in col.lower() or 'sentence' in col.lower()]\n",
    "        if text_columns:\n",
    "            print(f\"Text columns found: {text_columns}\")\n",
    "            for col in text_columns[:1]:  # Show first text column\n",
    "                sample_text = df[col].iloc[0] if len(df) > 0 else 'N/A'\n",
    "                print(f\"Sample {col}: {sample_text}\")\n",
    "        \n",
    "        # Show speaker ID info if available\n",
    "        if 'speaker_id' in df.columns:\n",
    "            unique_speakers = df['speaker_id'].nunique()\n",
    "            print(f\"Unique speakers: {unique_speakers}\")\n",
    "    \n",
    "    print(f\"\\nLibriSpeech dataset successfully saved to: {librispeech_data_dir.absolute()}\")\n",
    "else:\n",
    "    print(\"LibriSpeech dataset not loaded. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d53df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Evaluation Framework Dataset Helper\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Import evaluation framework components\n",
    "from config import get_settings\n",
    "from dataset_manager import (\n",
    "    list_available_datasets, \n",
    "    get_dataset_status,\n",
    "    setup_eval_datasets,\n",
    "    download_datasets\n",
    ")\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load evaluation settings\n",
    "settings = get_settings()\n",
    "print(f\"ğŸ“‹ Evaluation Framework Configuration Loaded\")\n",
    "print(f\"Dataset directory: {settings.datasets.base_download_dir}\")\n",
    "print(f\"Cache directory: {settings.datasets.cache_dir}\")\n",
    "print(f\"Sample size: {settings.datasets.sample_size}\")\n",
    "print()\n",
    "\n",
    "# List all available datasets\n",
    "print(\"ğŸ—‚ï¸ Available Datasets:\")\n",
    "print(\"=\" * 60)\n",
    "available_datasets = list_available_datasets()\n",
    "for key, info in available_datasets.items():\n",
    "    print(f\"ğŸ“Š {key}:\")\n",
    "    print(f\"    Name: {info['name']}\")\n",
    "    print(f\"    Task: {info['task']}\")\n",
    "    print(f\"    Language: {info['language']}\")\n",
    "    print(f\"    Repository: {info['repo_id']}\")\n",
    "    print(f\"    Description: {info['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798afe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current dataset status\n",
    "print(\"ğŸ“ˆ Current Dataset Status:\")\n",
    "print(\"=\" * 50)\n",
    "status = get_dataset_status(settings)\n",
    "for key, info in status.items():\n",
    "    if info['downloaded']:\n",
    "        print(f\"âœ… {info['name']} ({key})\")\n",
    "        if 'total_samples' in info:\n",
    "            print(f\"    ğŸ“Š Total samples: {info['total_samples']:,}\")\n",
    "        if 'splits' in info:\n",
    "            print(f\"    ğŸ“ Splits: {', '.join(info['splits'].keys())}\")\n",
    "    else:\n",
    "        print(f\"âŒ {info['name']} ({key}) - Not downloaded\")\n",
    "        if 'error' in info:\n",
    "            print(f\"    âš ï¸  Error: {info['error']}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0716191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download recommended datasets for evaluation\n",
    "print(\"ğŸš€ Setting up recommended evaluation datasets...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# This will download Arabic Common Voice, LibriSpeech Clean, and CallHome Diarization\n",
    "recommended_results = setup_eval_datasets(quick_setup=True, settings=settings)\n",
    "\n",
    "print(\"\\nğŸ“‹ Download Results:\")\n",
    "total_samples = 0\n",
    "for key, dataset in recommended_results.items():\n",
    "    if dataset:\n",
    "        available_datasets = list_available_datasets()\n",
    "        config_info = available_datasets[key]\n",
    "        dataset_samples = sum(len(split_data) for split_data in dataset.values()) if dataset else 0\n",
    "        total_samples += dataset_samples\n",
    "        \n",
    "        print(f\"âœ… {config_info['name']} ({key}):\")\n",
    "        print(f\"    ğŸ“Š Total samples: {dataset_samples:,}\")\n",
    "        print(f\"    ğŸ“ Splits: {', '.join(dataset.keys())}\")\n",
    "        print(f\"    ğŸ·ï¸  Task: {config_info['task']}\")\n",
    "        print(f\"    ğŸŒ Language: {config_info['language']}\")\n",
    "    else:\n",
    "        print(f\"âŒ {key}: Failed to download\")\n",
    "\n",
    "print(f\"\\nğŸ† Total samples across all datasets: {total_samples:,}\")\n",
    "print(\"ğŸ“‚ Datasets saved to:\", settings.datasets.base_download_dir.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d476a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download specific task datasets using the helper\n",
    "print(\"ğŸ¯ Task-specific Dataset Downloads\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Download only STT datasets\n",
    "print(\"ğŸ“± Downloading Speech-to-Text datasets...\")\n",
    "stt_results = download_datasets(task=\"stt\", settings=settings)\n",
    "\n",
    "print(f\"\\nğŸ“Š STT Results:\")\n",
    "stt_total = 0\n",
    "for key, dataset in stt_results.items():\n",
    "    if dataset:\n",
    "        samples = sum(len(split_data) for split_data in dataset.values())\n",
    "        stt_total += samples\n",
    "        print(f\"  âœ… {key}: {samples:,} samples\")\n",
    "    else:\n",
    "        print(f\"  âŒ {key}: Failed\")\n",
    "\n",
    "print(f\"ğŸ“ˆ Total STT samples: {stt_total:,}\")\n",
    "\n",
    "# Show directory structure\n",
    "print(f\"\\nğŸ“ Dataset Directory Structure:\")\n",
    "datasets_dir = settings.datasets.base_download_dir\n",
    "if datasets_dir.exists():\n",
    "    for task_dir in sorted(datasets_dir.iterdir()):\n",
    "        if task_dir.is_dir():\n",
    "            print(f\"  ğŸ“‚ {task_dir.name}/\")\n",
    "            for dataset_dir in sorted(task_dir.iterdir()):\n",
    "                if dataset_dir.is_dir():\n",
    "                    print(f\"    ğŸ“‚ {dataset_dir.name}/\")\n",
    "                    for split_dir in sorted(dataset_dir.iterdir()):\n",
    "                        if split_dir.is_dir():\n",
    "                            parquet_files = list(split_dir.glob(\"*.parquet\"))\n",
    "                            json_files = list(split_dir.glob(\"*.json\"))\n",
    "                            if parquet_files or json_files:\n",
    "                                print(f\"      ğŸ“ {split_dir.name}/ ({len(parquet_files)} parquet, {len(json_files)} json)\")\n",
    "                elif dataset_dir.suffix == '.json':\n",
    "                    print(f\"    ğŸ“„ {dataset_dir.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72954329",
   "metadata": {},
   "source": [
    "# Speech-to-Text (STT) Evaluation\n",
    "\n",
    "This section demonstrates how to use the STT evaluation framework implemented according to ADR-003 specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc31bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in /workspaces/oryx-cap-upskilling/.venv/lib/python3.13/site-packages (4.0.0)\n",
      "Requirement already satisfied: rapidfuzz in /workspaces/oryx-cap-upskilling/.venv/lib/python3.13/site-packages (3.14.1)\n",
      "Requirement already satisfied: pydantic in /workspaces/oryx-cap-upskilling/.venv/lib/python3.13/site-packages (2.12.2)\n",
      "Requirement already satisfied: click>=8.1.8 in /workspaces/oryx-cap-upskilling/.venv/lib/python3.13/site-packages (from jiwer) (8.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspaces/oryx-cap-upskilling/.venv/lib/python3.13/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /workspaces/oryx-cap-upskilling/.venv/lib/python3.13/site-packages (from pydantic) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /workspaces/oryx-cap-upskilling/.venv/lib/python3.13/site-packages (from pydantic) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /workspaces/oryx-cap-upskilling/.venv/lib/python3.13/site-packages (from pydantic) (0.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ¤ STT Evaluation Framework\n",
      "==================================================\n",
      "âœ… Successfully imported STT evaluation components\n",
      "ğŸ“‹ Available metrics: WER, MER, WIP, WIL, CER, LD, NLD\n",
      "ğŸŒ Supported languages: English (en), Arabic (ar)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for STT evaluation\n",
    "%pip install jiwer rapidfuzz pydantic\n",
    "\n",
    "# Import the STT evaluation framework with correct path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to path to access eval modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Import STT evaluation components\n",
    "from eval.stt.main import SpeechToTextEvaluator, evaluate_asr_model\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ¤ STT Evaluation Framework\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… Successfully imported STT evaluation components\")\n",
    "print(\"ğŸ“‹ Available metrics: WER, MER, WIP, WIL, CER, LD, NLD\")\n",
    "print(\"ğŸŒ Supported languages: English (en), Arabic (ar)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f63de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Sample STT Evaluation Data Structure\n",
      "==================================================\n",
      "âœ… English evaluation data structure created\n",
      "   ğŸ“Š 5 samples\n",
      "   ğŸ¯ Domain: read_speech\n",
      "âœ… Arabic evaluation data structure created\n",
      "   ğŸ“Š 4 samples\n",
      "   ğŸŒ Dialect: msa\n",
      "\n",
      "ğŸ“‹ Sample Data Structure:\n",
      "{\n",
      "  \"id\": \"librispeech_001\",\n",
      "  \"audio_file\": \"librispeech/test-clean/1089/134686/1089-134686-0000.flac\",\n",
      "  \"reference\": \"he began a confused complaint against the wizard who had vanished behind the curtain on the left\",\n",
      "  \"hypothesis\": \"he began a confused complaint against the wizard who had vanished behind the curtain on the left\",\n",
      "  \"metadata\": {\n",
      "    \"speaker_id\": \"1089\",\n",
      "    \"duration_ms\": 4280,\n",
      "    \"audio_quality\": \"studio\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create sample evaluation data structure following ADR-003 specifications\n",
    "print(\"ğŸ“Š Sample STT Evaluation Data Structure\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: English evaluation data\n",
    "english_evaluation_data = {\n",
    "    \"dataset_info\": {\n",
    "        \"name\": \"LibriSpeech Test-Clean\",\n",
    "        \"language\": \"en\", \n",
    "        \"domain\": \"read_speech\",\n",
    "        \"total_samples\": 5,\n",
    "        \"speaker_count\": 3\n",
    "    },\n",
    "    \"samples\": [\n",
    "        {\n",
    "            \"id\": \"librispeech_001\",\n",
    "            \"audio_file\": \"librispeech/test-clean/1089/134686/1089-134686-0000.flac\",\n",
    "            \"reference\": \"he began a confused complaint against the wizard who had vanished behind the curtain on the left\",\n",
    "            \"hypothesis\": \"he began a confused complaint against the wizard who had vanished behind the curtain on the left\",\n",
    "            \"metadata\": {\n",
    "                \"speaker_id\": \"1089\",\n",
    "                \"duration_ms\": 4280,\n",
    "                \"audio_quality\": \"studio\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"librispeech_002\", \n",
    "            \"reference\": \"there was a woman on the bed unconscious lying on her back\",\n",
    "            \"hypothesis\": \"there was a women on the bed unconscious lying on her back\",\n",
    "            \"metadata\": {\n",
    "                \"speaker_id\": \"1089\",\n",
    "                \"duration_ms\": 3120\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"librispeech_003\",\n",
    "            \"reference\": \"the horse galloped across the open field with tremendous speed\",\n",
    "            \"hypothesis\": \"the horse galloped across the open field with tremendus speed\",\n",
    "            \"metadata\": {\n",
    "                \"speaker_id\": \"2830\",\n",
    "                \"duration_ms\": 2950\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"medical_001\",\n",
    "            \"reference\": \"patient reports chest pain radiating to the left arm\",\n",
    "            \"hypothesis\": \"patient reports chest pain radiating to the left arm\",\n",
    "            \"metadata\": {\n",
    "                \"domain\": \"medical\",\n",
    "                \"speaker_role\": \"doctor\",\n",
    "                \"duration_ms\": 2100\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"noisy_001\",\n",
    "            \"reference\": \"the meeting will begin at three thirty this afternoon\",\n",
    "            \"hypothesis\": \"the meeting will begin at free thirty this afternoon\",\n",
    "            \"metadata\": {\n",
    "                \"noise_type\": \"background_chatter\",\n",
    "                \"snr_db\": 10.5,\n",
    "                \"audio_quality\": \"noisy\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Example 2: Arabic evaluation data  \n",
    "arabic_evaluation_data = {\n",
    "    \"dataset_info\": {\n",
    "        \"name\": \"Arabic Common Voice + MGB-3\",\n",
    "        \"language\": \"ar\",\n",
    "        \"dialect\": \"msa\",  # Modern Standard Arabic\n",
    "        \"domain\": \"mixed\",\n",
    "        \"total_samples\": 4\n",
    "    },\n",
    "    \"samples\": [\n",
    "        {\n",
    "            \"id\": \"commonvoice_ar_001\",\n",
    "            \"reference\": \"Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ\",\n",
    "            \"hypothesis\": \"Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ\",\n",
    "            \"metadata\": {\n",
    "                \"dataset\": \"common_voice_ar\",\n",
    "                \"speaker_id\": \"cv_ar_001\", \n",
    "                \"duration_ms\": 2400,\n",
    "                \"diacritics\": False,\n",
    "                \"dialect\": \"msa\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"mgb3_001\",\n",
    "            \"reference\": \"Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ… ØªØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ\",\n",
    "            \"hypothesis\": \"Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ… ØªØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠ\",\n",
    "            \"metadata\": {\n",
    "                \"dataset\": \"mgb3\",\n",
    "                \"domain\": \"broadcast_news\",\n",
    "                \"channel\": \"Al Jazeera\",\n",
    "                \"duration_ms\": 3200\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"medical_ar_001\", \n",
    "            \"reference\": \"Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØ´ÙƒÙˆ Ù…Ù† Ø£Ù„Ù… ÙÙŠ Ø§Ù„ØµØ¯Ø±\",\n",
    "            \"hypothesis\": \"Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØ´ÙƒÙˆ Ù…Ù† Ø£Ù„Ù… ÙÙŠ Ø§Ù„ØµØ¯Ø±\",\n",
    "            \"metadata\": {\n",
    "                \"domain\": \"medical\",\n",
    "                \"speaker_role\": \"patient\",\n",
    "                \"duration_ms\": 1800,\n",
    "                \"dialect\": \"levantine\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"codeswitching_001\",\n",
    "            \"reference\": \"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… welcome to our English Arabic program\",\n",
    "            \"hypothesis\": \"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… welcome to our English Arabic program\", \n",
    "            \"metadata\": {\n",
    "                \"codeswitching\": True,\n",
    "                \"secondary_language\": \"en\",\n",
    "                \"domain\": \"broadcast\",\n",
    "                \"duration_ms\": 2800\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"âœ… English evaluation data structure created\")\n",
    "print(f\"   ğŸ“Š {len(english_evaluation_data['samples'])} samples\")\n",
    "print(f\"   ğŸ¯ Domain: {english_evaluation_data['dataset_info']['domain']}\")\n",
    "\n",
    "print(\"âœ… Arabic evaluation data structure created\") \n",
    "print(f\"   ğŸ“Š {len(arabic_evaluation_data['samples'])} samples\")\n",
    "print(f\"   ğŸŒ Dialect: {arabic_evaluation_data['dataset_info']['dialect']}\")\n",
    "print()\n",
    "\n",
    "# Show sample data structure\n",
    "print(\"ğŸ“‹ Sample Data Structure:\")\n",
    "print(json.dumps(english_evaluation_data['samples'][0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26882c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‡ºğŸ‡¸ English STT Evaluation\n",
      "==================================================\n",
      "ğŸ“Š Evaluating 5 English samples...\n",
      "\n",
      "ğŸ”§ Text Normalization Examples:\n",
      "Original:   He said, \"Hello there!\" â€” it's wonderful.\n",
      "Normalized: he said, \"hello there!\" - it's wonderful.\n",
      "\n",
      "ğŸ“ˆ English STT Results:\n",
      "------------------------------\n",
      "ğŸ¯ WER (Word Error Rate):           0.0526\n",
      "ğŸ” MER (Match Error Rate):          0.0526\n",
      "âœ… WIP (Word Information Preserved): 0.8975\n",
      "âŒ WIL (Word Information Lost):     0.1025\n",
      "ğŸ”¤ CER (Character Error Rate):      0.0125\n",
      "ğŸ“ Word Levenshtein Distance:       0.60\n",
      "ğŸ“ Char Levenshtein Distance:       0.80\n",
      "ğŸ“Š Normalized Word LD:              0.0589\n",
      "ğŸ“‹ Normalized Char LD:              0.0142\n",
      "\n",
      "ğŸŒ Language: en\n",
      "ğŸ“Š Sample Count: 5\n",
      "\n",
      "ğŸ” Individual Sample Analysis:\n",
      "âœ… librispeech_001: Perfect match (WER = 0.000)\n",
      "âŒ librispeech_002: WER = 0.083\n",
      "   Reference: there was a woman on the bed unconscious lying on her back\n",
      "   Hypothesis: there was a women on the bed unconscious lying on her back\n",
      "âŒ librispeech_003: WER = 0.100\n",
      "   Reference: the horse galloped across the open field with tremendous speed\n",
      "   Hypothesis: the horse galloped across the open field with tremendus speed\n",
      "âœ… medical_001: Perfect match (WER = 0.000)\n",
      "âŒ noisy_001: WER = 0.111\n",
      "   Reference: the meeting will begin at three thirty this afternoon\n",
      "   Hypothesis: the meeting will begin at free thirty this afternoon\n"
     ]
    }
   ],
   "source": [
    "# Evaluate English STT samples\n",
    "print(\"ğŸ‡ºğŸ‡¸ English STT Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract references and hypotheses for evaluation\n",
    "english_references = [sample[\"reference\"] for sample in english_evaluation_data[\"samples\"]]\n",
    "english_hypotheses = [sample[\"hypothesis\"] for sample in english_evaluation_data[\"samples\"]]\n",
    "\n",
    "print(f\"ğŸ“Š Evaluating {len(english_references)} English samples...\")\n",
    "print()\n",
    "\n",
    "# Create English evaluator\n",
    "english_evaluator = SpeechToTextEvaluator(language=\"en\")\n",
    "\n",
    "# Show text normalization examples\n",
    "print(\"ğŸ”§ Text Normalization Examples:\")\n",
    "sample_text = 'He said, \"Hello there!\" â€” it\\'s wonderful.'\n",
    "normalized_text = english_evaluator.normalize_text(sample_text)\n",
    "print(f\"Original:   {sample_text}\")\n",
    "print(f\"Normalized: {normalized_text}\")\n",
    "print()\n",
    "\n",
    "# Perform comprehensive evaluation\n",
    "english_results = english_evaluator.evaluate_full(english_references, english_hypotheses)\n",
    "\n",
    "print(\"ğŸ“ˆ English STT Results:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"ğŸ¯ WER (Word Error Rate):           {english_results.wer:.4f}\")\n",
    "print(f\"ğŸ” MER (Match Error Rate):          {english_results.mer:.4f}\")  \n",
    "print(f\"âœ… WIP (Word Information Preserved): {english_results.wip:.4f}\")\n",
    "print(f\"âŒ WIL (Word Information Lost):     {english_results.wil:.4f}\")\n",
    "print(f\"ğŸ”¤ CER (Character Error Rate):      {english_results.cer:.4f}\")\n",
    "print(f\"ğŸ“ Word Levenshtein Distance:       {english_results.word_levenshtein_distance:.2f}\")\n",
    "print(f\"ğŸ“ Char Levenshtein Distance:       {english_results.char_levenshtein_distance:.2f}\")\n",
    "print(f\"ğŸ“Š Normalized Word LD:              {english_results.normalized_word_ld:.4f}\")\n",
    "print(f\"ğŸ“‹ Normalized Char LD:              {english_results.normalized_char_ld:.4f}\")\n",
    "print()\n",
    "print(f\"ğŸŒ Language: {english_results.language}\")\n",
    "print(f\"ğŸ“Š Sample Count: {english_results.sample_count}\")\n",
    "\n",
    "# Show individual sample errors\n",
    "print(\"\\nğŸ” Individual Sample Analysis:\")\n",
    "for i, (ref, hyp) in enumerate(zip(english_references, english_hypotheses)):\n",
    "    sample_id = english_evaluation_data[\"samples\"][i][\"id\"]\n",
    "    if ref != hyp:\n",
    "        # Calculate individual WER for this sample\n",
    "        sample_wer = english_evaluator._calculate_word_metrics([ref], [hyp])[\"wer\"]\n",
    "        print(f\"âŒ {sample_id}: WER = {sample_wer:.3f}\")\n",
    "        print(f\"   Reference: {ref}\")\n",
    "        print(f\"   Hypothesis: {hyp}\")\n",
    "    else:\n",
    "        print(f\"âœ… {sample_id}: Perfect match (WER = 0.000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510313ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‡¸ğŸ‡¦ Arabic STT Evaluation\n",
      "==================================================\n",
      "ğŸ“Š Evaluating 4 Arabic samples...\n",
      "\n",
      "ğŸ”§ Arabic Text Normalization Examples:\n",
      "Original (with diacritics): Ù…ÙØ±Ù’Ø­ÙØ¨Ø§Ù‹ Ø¨ÙÙƒÙ ÙÙÙŠ Ù‡Ù°Ø°ÙØ§ Ø§Ù„Ø¨ÙØ±Ù’Ù†ÙØ§Ù…ÙØ¬\n",
      "Normalized (no diacritics): Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬\n",
      "\n",
      "Original punctuation:   Ù‚Ø§Ù„ \"Ù…Ø±Ø­Ø¨Ø§\" â€” Ù‡Ø°Ø§ Ø±Ø§Ø¦Ø¹!\n",
      "Normalized punctuation: Ù‚Ø§Ù„ \"Ù…Ø±Ø­Ø¨Ø§\" - Ù‡Ø°Ø§ Ø±Ø§Ø¦Ø¹!\n",
      "\n",
      "ğŸ“ˆ Arabic STT Results:\n",
      "------------------------------\n",
      "ğŸ¯ WER (Word Error Rate):           0.0385\n",
      "ğŸ” MER (Match Error Rate):          0.0385\n",
      "âœ… WIP (Word Information Preserved): 0.9246\n",
      "âŒ WIL (Word Information Lost):     0.0754\n",
      "ğŸ”¤ CER (Character Error Rate):      0.0068\n",
      "ğŸ“ Word Levenshtein Distance:       0.25\n",
      "ğŸ“ Char Levenshtein Distance:       0.25\n",
      "ğŸ“Š Normalized Word LD:              0.0417\n",
      "ğŸ“‹ Normalized Char LD:              0.0066\n",
      "\n",
      "ğŸŒ Language: ar\n",
      "ğŸ“Š Sample Count: 4\n",
      "\n",
      "ğŸ” Individual Arabic Sample Analysis:\n",
      "âœ… commonvoice_ar_001: Perfect match after normalization\n",
      "   Dialect: msa\n",
      "âŒ mgb3_001: WER = 0.167\n",
      "   Reference: Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ… ØªØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ\n",
      "   Hypothesis: Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ… ØªØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠ\n",
      "   Normalized Ref: Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ… ØªØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ\n",
      "   Normalized Hyp: Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ… ØªØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠ\n",
      "âœ… medical_ar_001: Perfect match after normalization\n",
      "   Dialect: levantine\n",
      "âœ… codeswitching_001: Perfect match after normalization\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Arabic STT samples\n",
    "print(\"ğŸ‡¸ğŸ‡¦ Arabic STT Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract Arabic references and hypotheses\n",
    "arabic_references = [sample[\"reference\"] for sample in arabic_evaluation_data[\"samples\"]]\n",
    "arabic_hypotheses = [sample[\"hypothesis\"] for sample in arabic_evaluation_data[\"samples\"]]\n",
    "\n",
    "print(f\"ğŸ“Š Evaluating {len(arabic_references)} Arabic samples...\")\n",
    "print()\n",
    "\n",
    "# Create Arabic evaluator with diacritic stripping enabled\n",
    "arabic_evaluator = SpeechToTextEvaluator(language=\"ar\", strip_arabic_diacritics=True)\n",
    "\n",
    "# Show Arabic text normalization examples\n",
    "print(\"ğŸ”§ Arabic Text Normalization Examples:\")\n",
    "diacritic_text = \"Ù…ÙØ±Ù’Ø­ÙØ¨Ø§Ù‹ Ø¨ÙÙƒÙ ÙÙÙŠ Ù‡Ù°Ø°ÙØ§ Ø§Ù„Ø¨ÙØ±Ù’Ù†ÙØ§Ù…ÙØ¬\"\n",
    "normalized_arabic = arabic_evaluator.normalize_text(diacritic_text)\n",
    "print(f\"Original (with diacritics): {diacritic_text}\")\n",
    "print(f\"Normalized (no diacritics): {normalized_arabic}\")\n",
    "print()\n",
    "\n",
    "# Show punctuation normalization\n",
    "punct_text = 'Ù‚Ø§Ù„ \"Ù…Ø±Ø­Ø¨Ø§\" â€” Ù‡Ø°Ø§ Ø±Ø§Ø¦Ø¹!'\n",
    "normalized_punct = arabic_evaluator.normalize_text(punct_text)\n",
    "print(f\"Original punctuation:   {punct_text}\")\n",
    "print(f\"Normalized punctuation: {normalized_punct}\")\n",
    "print()\n",
    "\n",
    "# Perform comprehensive Arabic evaluation\n",
    "arabic_results = arabic_evaluator.evaluate_full(arabic_references, arabic_hypotheses)\n",
    "\n",
    "print(\"ğŸ“ˆ Arabic STT Results:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"ğŸ¯ WER (Word Error Rate):           {arabic_results.wer:.4f}\")\n",
    "print(f\"ğŸ” MER (Match Error Rate):          {arabic_results.mer:.4f}\")\n",
    "print(f\"âœ… WIP (Word Information Preserved): {arabic_results.wip:.4f}\")  \n",
    "print(f\"âŒ WIL (Word Information Lost):     {arabic_results.wil:.4f}\")\n",
    "print(f\"ğŸ”¤ CER (Character Error Rate):      {arabic_results.cer:.4f}\")\n",
    "print(f\"ğŸ“ Word Levenshtein Distance:       {arabic_results.word_levenshtein_distance:.2f}\")\n",
    "print(f\"ğŸ“ Char Levenshtein Distance:       {arabic_results.char_levenshtein_distance:.2f}\")\n",
    "print(f\"ğŸ“Š Normalized Word LD:              {arabic_results.normalized_word_ld:.4f}\")\n",
    "print(f\"ğŸ“‹ Normalized Char LD:              {arabic_results.normalized_char_ld:.4f}\")\n",
    "print()\n",
    "print(f\"ğŸŒ Language: {arabic_results.language}\")\n",
    "print(f\"ğŸ“Š Sample Count: {arabic_results.sample_count}\")\n",
    "\n",
    "# Show individual Arabic sample analysis\n",
    "print(\"\\nğŸ” Individual Arabic Sample Analysis:\")\n",
    "for i, (ref, hyp) in enumerate(zip(arabic_references, arabic_hypotheses)):\n",
    "    sample_id = arabic_evaluation_data[\"samples\"][i][\"id\"]\n",
    "    metadata = arabic_evaluation_data[\"samples\"][i][\"metadata\"]\n",
    "    \n",
    "    # Normalize both for comparison\n",
    "    norm_ref = arabic_evaluator.normalize_text(ref)\n",
    "    norm_hyp = arabic_evaluator.normalize_text(hyp)\n",
    "    \n",
    "    if norm_ref != norm_hyp:\n",
    "        sample_wer = arabic_evaluator._calculate_word_metrics([ref], [hyp])[\"wer\"] \n",
    "        print(f\"âŒ {sample_id}: WER = {sample_wer:.3f}\")\n",
    "        print(f\"   Reference: {ref}\")\n",
    "        print(f\"   Hypothesis: {hyp}\")\n",
    "        print(f\"   Normalized Ref: {norm_ref}\")\n",
    "        print(f\"   Normalized Hyp: {norm_hyp}\")\n",
    "        if \"dialect\" in metadata:\n",
    "            print(f\"   Dialect: {metadata['dialect']}\")\n",
    "    else:\n",
    "        print(f\"âœ… {sample_id}: Perfect match after normalization\")\n",
    "        if \"dialect\" in metadata:\n",
    "            print(f\"   Dialect: {metadata['dialect']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4340d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Comparative Analysis Across Domains\n",
      "==================================================\n",
      "ğŸ‡ºğŸ‡¸ English Performance by Domain:\n",
      "  ğŸ“‚ General:\n",
      "     ğŸ¯ WER: 0.062\n",
      "     ğŸ”¤ CER: 0.015\n",
      "     ğŸ“Š Samples: 4\n",
      "  ğŸ“‚ Medical:\n",
      "     ğŸ¯ WER: 0.000\n",
      "     ğŸ”¤ CER: 0.000\n",
      "     ğŸ“Š Samples: 1\n",
      "\n",
      "ğŸ‡¸ğŸ‡¦ Arabic Performance by Dialect:\n",
      "  ğŸ—£ï¸  MSA:\n",
      "     ğŸ¯ WER: 0.000\n",
      "     ğŸ”¤ CER: 0.000\n",
      "     ğŸ“Š Samples: 1\n",
      "  ğŸ—£ï¸  UNKNOWN:\n",
      "     ğŸ¯ WER: 0.071\n",
      "     ğŸ”¤ CER: 0.011\n",
      "     ğŸ“Š Samples: 2\n",
      "  ğŸ—£ï¸  LEVANTINE:\n",
      "     ğŸ¯ WER: 0.000\n",
      "     ğŸ”¤ CER: 0.000\n",
      "     ğŸ“Š Samples: 1\n",
      "\n",
      "âš ï¸  Error Analysis - Most Challenging Cases:\n",
      "---------------------------------------------\n",
      "ğŸ”´ mgb3_001 (AR):\n",
      "   WER: 0.167, CER: 0.026\n",
      "   Error Type: substitution\n",
      "   Ref: Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ… ØªØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ\n",
      "   Hyp: Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ… ØªØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠ\n",
      "\n",
      "ğŸ”´ noisy_001 (EN):\n",
      "   WER: 0.111, CER: 0.038\n",
      "   Error Type: substitution\n",
      "   Ref: the meeting will begin at three thirty this afternoon\n",
      "   Hyp: the meeting will begin at free thirty this afternoon\n",
      "\n",
      "ğŸ”´ librispeech_003 (EN):\n",
      "   WER: 0.100, CER: 0.016\n",
      "   Error Type: substitution\n",
      "   Ref: the horse galloped across the open field with tremendous speed\n",
      "   Hyp: the horse galloped across the open field with tremendus speed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare performance across different domains and conditions\n",
    "print(\"ğŸ“Š Comparative Analysis Across Domains\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze by domain (English)\n",
    "english_by_domain = {}\n",
    "for i, sample in enumerate(english_evaluation_data[\"samples\"]):\n",
    "    domain = sample[\"metadata\"].get(\"domain\", \"general\")\n",
    "    if domain not in english_by_domain:\n",
    "        english_by_domain[domain] = {\"refs\": [], \"hyps\": [], \"ids\": []}\n",
    "    \n",
    "    english_by_domain[domain][\"refs\"].append(sample[\"reference\"])\n",
    "    english_by_domain[domain][\"hyps\"].append(sample[\"hypothesis\"])\n",
    "    english_by_domain[domain][\"ids\"].append(sample[\"id\"])\n",
    "\n",
    "print(\"ğŸ‡ºğŸ‡¸ English Performance by Domain:\")\n",
    "for domain, data in english_by_domain.items():\n",
    "    if len(data[\"refs\"]) > 0:\n",
    "        domain_results = english_evaluator.evaluate_full(data[\"refs\"], data[\"hyps\"])\n",
    "        print(f\"  ğŸ“‚ {domain.title()}:\")\n",
    "        print(f\"     ğŸ¯ WER: {domain_results.wer:.3f}\")\n",
    "        print(f\"     ğŸ”¤ CER: {domain_results.cer:.3f}\")\n",
    "        print(f\"     ğŸ“Š Samples: {len(data['refs'])}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Analyze by dialect (Arabic)\n",
    "arabic_by_dialect = {}\n",
    "for i, sample in enumerate(arabic_evaluation_data[\"samples\"]):\n",
    "    dialect = sample[\"metadata\"].get(\"dialect\", \"unknown\")\n",
    "    if dialect not in arabic_by_dialect:\n",
    "        arabic_by_dialect[dialect] = {\"refs\": [], \"hyps\": [], \"ids\": []}\n",
    "    \n",
    "    arabic_by_dialect[dialect][\"refs\"].append(sample[\"reference\"])\n",
    "    arabic_by_dialect[dialect][\"hyps\"].append(sample[\"hypothesis\"])\n",
    "    arabic_by_dialect[dialect][\"ids\"].append(sample[\"id\"])\n",
    "\n",
    "print(\"ğŸ‡¸ğŸ‡¦ Arabic Performance by Dialect:\")\n",
    "for dialect, data in arabic_by_dialect.items():\n",
    "    if len(data[\"refs\"]) > 0:\n",
    "        dialect_results = arabic_evaluator.evaluate_full(data[\"refs\"], data[\"hyps\"])\n",
    "        print(f\"  ğŸ—£ï¸  {dialect.upper()}:\")\n",
    "        print(f\"     ğŸ¯ WER: {dialect_results.wer:.3f}\")\n",
    "        print(f\"     ğŸ”¤ CER: {dialect_results.cer:.3f}\")\n",
    "        print(f\"     ğŸ“Š Samples: {len(data['refs'])}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show challenging cases analysis\n",
    "print(\"âš ï¸  Error Analysis - Most Challenging Cases:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Find samples with highest error rates\n",
    "all_samples = []\n",
    "\n",
    "# Add English samples\n",
    "for i, sample in enumerate(english_evaluation_data[\"samples\"]):\n",
    "    ref, hyp = sample[\"reference\"], sample[\"hypothesis\"]\n",
    "    if ref != hyp:\n",
    "        sample_metrics = english_evaluator._calculate_word_metrics([ref], [hyp])\n",
    "        all_samples.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"language\": \"en\", \n",
    "            \"wer\": sample_metrics[\"wer\"],\n",
    "            \"cer\": english_evaluator._calculate_cer([ref], [hyp]),\n",
    "            \"reference\": ref,\n",
    "            \"hypothesis\": hyp,\n",
    "            \"error_type\": \"substitution\" if len(ref.split()) == len(hyp.split()) else \"length_mismatch\"\n",
    "        })\n",
    "\n",
    "# Add Arabic samples  \n",
    "for i, sample in enumerate(arabic_evaluation_data[\"samples\"]):\n",
    "    ref, hyp = sample[\"reference\"], sample[\"hypothesis\"]\n",
    "    norm_ref = arabic_evaluator.normalize_text(ref)\n",
    "    norm_hyp = arabic_evaluator.normalize_text(hyp)\n",
    "    if norm_ref != norm_hyp:\n",
    "        sample_metrics = arabic_evaluator._calculate_word_metrics([ref], [hyp])\n",
    "        all_samples.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"language\": \"ar\",\n",
    "            \"wer\": sample_metrics[\"wer\"],\n",
    "            \"cer\": arabic_evaluator._calculate_cer([ref], [hyp]),\n",
    "            \"reference\": ref,\n",
    "            \"hypothesis\": hyp,\n",
    "            \"error_type\": \"diacritic\" if ref != hyp and norm_ref == norm_hyp else \"substitution\"\n",
    "        })\n",
    "\n",
    "# Sort by WER (highest first)\n",
    "all_samples.sort(key=lambda x: x[\"wer\"], reverse=True)\n",
    "\n",
    "for sample in all_samples[:3]:  # Show top 3 challenging cases\n",
    "    print(f\"ğŸ”´ {sample['id']} ({sample['language'].upper()}):\")\n",
    "    print(f\"   WER: {sample['wer']:.3f}, CER: {sample['cer']:.3f}\")\n",
    "    print(f\"   Error Type: {sample['error_type']}\")\n",
    "    print(f\"   Ref: {sample['reference']}\")\n",
    "    print(f\"   Hyp: {sample['hypothesis']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257caa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate real sample validation datasets\n",
    "print(\"ğŸ“ Loading Sample Validation Datasets\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load English STT sample dataset\n",
    "english_sample_path = Path(\"../stt/sample_datasets/english_stt_validation.jsonl\")\n",
    "arabic_sample_path = Path(\"../stt/sample_datasets/arabic_stt_validation.jsonl\")\n",
    "\n",
    "if english_sample_path.exists():\n",
    "    print(\"âœ… Loading English validation dataset...\")\n",
    "    with open(english_sample_path, 'r', encoding='utf-8') as f:\n",
    "        english_samples = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"ğŸ“Š Loaded {len(english_samples)} English samples\")\n",
    "    \n",
    "    # Extract data for evaluation\n",
    "    english_val_refs = [sample[\"reference\"] for sample in english_samples]\n",
    "    english_val_hyps = [sample[\"hypothesis\"] for sample in english_samples]\n",
    "    \n",
    "    # Evaluate\n",
    "    english_val_results = english_evaluator.evaluate_full(english_val_refs, english_val_hyps)\n",
    "    \n",
    "    print(\"ğŸ“ˆ English Validation Results:\")\n",
    "    print(f\"   ğŸ¯ WER: {english_val_results.wer:.4f}\")\n",
    "    print(f\"   ğŸ”¤ CER: {english_val_results.cer:.4f}\")\n",
    "    print(f\"   âœ… WIP: {english_val_results.wip:.4f}\")\n",
    "    print(f\"   ğŸ“Š Total Samples: {english_val_results.sample_count}\")\n",
    "    \n",
    "    # Show dataset composition\n",
    "    datasets_used = set(sample[\"metadata\"][\"dataset\"] for sample in english_samples)\n",
    "    print(f\"   ğŸ“‚ Datasets: {', '.join(datasets_used)}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"âŒ English validation dataset not found\")\n",
    "\n",
    "if arabic_sample_path.exists():\n",
    "    print(\"âœ… Loading Arabic validation dataset...\")\n",
    "    with open(arabic_sample_path, 'r', encoding='utf-8') as f:\n",
    "        arabic_samples = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"ğŸ“Š Loaded {len(arabic_samples)} Arabic samples\")\n",
    "    \n",
    "    # Extract data for evaluation\n",
    "    arabic_val_refs = [sample[\"reference\"] for sample in arabic_samples]\n",
    "    arabic_val_hyps = [sample[\"hypothesis\"] for sample in arabic_samples]\n",
    "    \n",
    "    # Evaluate\n",
    "    arabic_val_results = arabic_evaluator.evaluate_full(arabic_val_refs, arabic_val_hyps)\n",
    "    \n",
    "    print(\"ğŸ“ˆ Arabic Validation Results:\")\n",
    "    print(f\"   ğŸ¯ WER: {arabic_val_results.wer:.4f}\")\n",
    "    print(f\"   ğŸ”¤ CER: {arabic_val_results.cer:.4f}\")\n",
    "    print(f\"   âœ… WIP: {arabic_val_results.wip:.4f}\")\n",
    "    print(f\"   ğŸ“Š Total Samples: {arabic_val_results.sample_count}\")\n",
    "    \n",
    "    # Show dialect distribution\n",
    "    dialects = [sample[\"metadata\"].get(\"dialect\", \"unknown\") for sample in arabic_samples]\n",
    "    dialect_counts = {d: dialects.count(d) for d in set(dialects)}\n",
    "    print(f\"   ğŸ—£ï¸  Dialects: {dict(dialect_counts)}\")\n",
    "    \n",
    "    # Show datasets used\n",
    "    datasets_used = set(sample[\"metadata\"][\"dataset\"] for sample in arabic_samples)\n",
    "    print(f\"   ğŸ“‚ Datasets: {', '.join(datasets_used)}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"âŒ Arabic validation dataset not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d87b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced evaluation scenarios and metric interpretation\n",
    "print(\"ğŸ”¬ Advanced STT Evaluation Scenarios\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Scenario 1: Different normalization settings impact\n",
    "print(\"1ï¸âƒ£ Impact of Text Normalization Settings\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_samples = [\n",
    "    (\"The user said \"Hello!\" â€” that's great.\", \"The user said 'Hello!' - that's great.\"),\n",
    "    (\"Ù…ÙØ±Ù’Ø­ÙØ¨Ø§Ù‹ Ø¨ÙÙƒÙ\", \"Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ\"),  # Arabic with vs without diacritics\n",
    "    (\"Dr. Smith's appointment is at 3:30 P.M.\", \"doctor smiths appointment is at three thirty pm\")\n",
    "]\n",
    "\n",
    "# Test with different normalization settings\n",
    "for i, (ref, hyp) in enumerate(test_samples, 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Reference:  {ref}\")\n",
    "    print(f\"Hypothesis: {hyp}\")\n",
    "    \n",
    "    # Determine language\n",
    "    lang = \"ar\" if any('\\u0600' <= char <= '\\u06FF' for char in ref) else \"en\"\n",
    "    \n",
    "    # With normalization\n",
    "    evaluator_norm = SpeechToTextEvaluator(language=lang, normalize_punctuation=True)\n",
    "    result_norm = evaluator_norm.evaluate_full([ref], [hyp])\n",
    "    \n",
    "    # Without normalization (for comparison where applicable)\n",
    "    evaluator_no_norm = SpeechToTextEvaluator(language=lang, normalize_punctuation=False)\n",
    "    result_no_norm = evaluator_no_norm.evaluate_full([ref], [hyp])\n",
    "    \n",
    "    print(f\"WER (with normalization):    {result_norm.wer:.3f}\")\n",
    "    print(f\"WER (without normalization): {result_no_norm.wer:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Scenario 2: Metric interpretation guide\n",
    "print(\"\\n2ï¸âƒ£ STT Metrics Interpretation Guide\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "interpretation_guide = {\n",
    "    \"WER\": {\n",
    "        \"excellent\": (0.00, 0.05, \"ğŸŸ¢ Excellent (0-5%)\"),\n",
    "        \"good\": (0.05, 0.15, \"ğŸŸ¡ Good (5-15%)\"), \n",
    "        \"acceptable\": (0.15, 0.30, \"ğŸŸ  Acceptable (15-30%)\"),\n",
    "        \"poor\": (0.30, 1.00, \"ğŸ”´ Poor (30%+)\")\n",
    "    },\n",
    "    \"CER\": {\n",
    "        \"excellent\": (0.00, 0.02, \"ğŸŸ¢ Excellent (0-2%)\"),\n",
    "        \"good\": (0.02, 0.08, \"ğŸŸ¡ Good (2-8%)\"),\n",
    "        \"acceptable\": (0.08, 0.20, \"ğŸŸ  Acceptable (8-20%)\"), \n",
    "        \"poor\": (0.20, 1.00, \"ğŸ”´ Poor (20%+)\")\n",
    "    }\n",
    "}\n",
    "\n",
    "def interpret_metric(value, metric_name):\n",
    "    \"\"\"Interpret a metric value according to standard benchmarks.\"\"\"\n",
    "    ranges = interpretation_guide.get(metric_name, {})\n",
    "    for category, (min_val, max_val, description) in ranges.items():\n",
    "        if min_val <= value < max_val:\n",
    "            return description\n",
    "    return \"ğŸ”´ Poor\"\n",
    "\n",
    "# Apply interpretation to our results\n",
    "print(\"ğŸ“Š Results Interpretation:\")\n",
    "print(f\"English Overall WER: {english_results.wer:.3f} â†’ {interpret_metric(english_results.wer, 'WER')}\")\n",
    "print(f\"English Overall CER: {english_results.cer:.3f} â†’ {interpret_metric(english_results.cer, 'CER')}\")\n",
    "print(f\"Arabic Overall WER:  {arabic_results.wer:.3f} â†’ {interpret_metric(arabic_results.wer, 'WER')}\")\n",
    "print(f\"Arabic Overall CER:  {arabic_results.cer:.3f} â†’ {interpret_metric(arabic_results.cer, 'CER')}\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Metric Relationships and Insights\")\n",
    "print(\"-\" * 40)\n",
    "print(\"ğŸ“‹ Key Insights:\")\n",
    "print(\"â€¢ WER focuses on word-level errors (good for content accuracy)\")\n",
    "print(\"â€¢ CER is more sensitive to spelling/orthographic errors\")  \n",
    "print(\"â€¢ MER complements WER by considering matching subsequences\")\n",
    "print(\"â€¢ WIP/WIL provide information-theoretic perspective\")\n",
    "print(\"â€¢ For Arabic: CER is especially important due to morphology\")\n",
    "print(\"â€¢ Diacritic handling significantly affects Arabic scores\")\n",
    "print(f\"â€¢ Normalization impact: {abs(result_norm.wer - result_no_norm.wer):.3f} avg WER difference\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Production Usage Patterns\")\n",
    "print(\"-\" * 40)\n",
    "print(\"ğŸš€ Recommended evaluation workflow:\")\n",
    "print(\"1. Load reference transcriptions and ASR hypotheses\")\n",
    "print(\"2. Choose appropriate language and normalization settings\")\n",
    "print(\"3. Run evaluate_asr_model() for comprehensive metrics\")\n",
    "print(\"4. Analyze results by domain/dialect for insights\")\n",
    "print(\"5. Focus on WER for primary quality assessment\")\n",
    "print(\"6. Use CER for detailed orthographic analysis\")\n",
    "print(\"7. Monitor WIP/WIL for information preservation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evaluation results for reporting\n",
    "print(\"ğŸ“„ Export Evaluation Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comprehensive evaluation report\n",
    "evaluation_report = {\n",
    "    \"evaluation_metadata\": {\n",
    "        \"timestamp\": \"2025-10-27T08:59:00Z\",\n",
    "        \"framework_version\": \"1.0.0\",\n",
    "        \"adr_compliance\": \"ADR-003\",\n",
    "        \"evaluator\": \"STT Evaluation Framework\"\n",
    "    },\n",
    "    \"english_evaluation\": {\n",
    "        \"language\": english_results.language,\n",
    "        \"sample_count\": english_results.sample_count,\n",
    "        \"metrics\": {\n",
    "            \"wer\": float(english_results.wer),\n",
    "            \"mer\": float(english_results.mer), \n",
    "            \"wip\": float(english_results.wip),\n",
    "            \"wil\": float(english_results.wil),\n",
    "            \"cer\": float(english_results.cer),\n",
    "            \"word_levenshtein_distance\": float(english_results.word_levenshtein_distance),\n",
    "            \"char_levenshtein_distance\": float(english_results.char_levenshtein_distance),\n",
    "            \"normalized_word_ld\": float(english_results.normalized_word_ld),\n",
    "            \"normalized_char_ld\": float(english_results.normalized_char_ld)\n",
    "        },\n",
    "        \"quality_assessment\": interpret_metric(english_results.wer, \"WER\"),\n",
    "        \"dataset_info\": english_evaluation_data[\"dataset_info\"]\n",
    "    },\n",
    "    \"arabic_evaluation\": {\n",
    "        \"language\": arabic_results.language,\n",
    "        \"sample_count\": arabic_results.sample_count,\n",
    "        \"metrics\": {\n",
    "            \"wer\": float(arabic_results.wer),\n",
    "            \"mer\": float(arabic_results.mer),\n",
    "            \"wip\": float(arabic_results.wip), \n",
    "            \"wil\": float(arabic_results.wil),\n",
    "            \"cer\": float(arabic_results.cer),\n",
    "            \"word_levenshtein_distance\": float(arabic_results.word_levenshtein_distance),\n",
    "            \"char_levenshtein_distance\": float(arabic_results.char_levenshtein_distance),\n",
    "            \"normalized_word_ld\": float(arabic_results.normalized_word_ld),\n",
    "            \"normalized_char_ld\": float(arabic_results.normalized_char_ld)\n",
    "        },\n",
    "        \"quality_assessment\": interpret_metric(arabic_results.wer, \"WER\"),\n",
    "        \"dataset_info\": arabic_evaluation_data[\"dataset_info\"],\n",
    "        \"normalization_settings\": {\n",
    "            \"strip_arabic_diacritics\": True,\n",
    "            \"normalize_punctuation\": True\n",
    "        }\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"total_samples\": english_results.sample_count + arabic_results.sample_count,\n",
    "        \"languages_evaluated\": [\"en\", \"ar\"],\n",
    "        \"domains_covered\": [\"read_speech\", \"medical\", \"broadcast_news\", \"conversational\"],\n",
    "        \"key_findings\": [\n",
    "            f\"English WER: {english_results.wer:.3f} ({interpret_metric(english_results.wer, 'WER')})\",\n",
    "            f\"Arabic WER: {arabic_results.wer:.3f} ({interpret_metric(arabic_results.wer, 'WER')})\",\n",
    "            f\"Cross-lingual CER comparison: EN={english_results.cer:.3f}, AR={arabic_results.cer:.3f}\",\n",
    "            \"Arabic diacritic normalization enabled for fair comparison\",\n",
    "            \"All ADR-003 metrics successfully computed\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save report to file\n",
    "report_path = Path(\"../stt_evaluation_report.json\")\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(evaluation_report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Evaluation report saved to: {report_path.absolute()}\")\n",
    "print(f\"ğŸ“Š Total samples evaluated: {evaluation_report['summary']['total_samples']}\")\n",
    "print(f\"ğŸŒ Languages: {', '.join(evaluation_report['summary']['languages_evaluated'])}\")\n",
    "print()\n",
    "\n",
    "# Display summary table\n",
    "print(\"ğŸ“‹ Summary Results Table:\")\n",
    "print(\"+\" + \"-\"*50 + \"+\")\n",
    "print(f\"| {'Metric':<15} | {'English':<15} | {'Arabic':<15} |\")\n",
    "print(\"+\" + \"-\"*50 + \"+\")\n",
    "print(f\"| {'WER':<15} | {english_results.wer:<15.4f} | {arabic_results.wer:<15.4f} |\")\n",
    "print(f\"| {'MER':<15} | {english_results.mer:<15.4f} | {arabic_results.mer:<15.4f} |\")\n",
    "print(f\"| {'CER':<15} | {english_results.cer:<15.4f} | {arabic_results.cer:<15.4f} |\")\n",
    "print(f\"| {'WIP':<15} | {english_results.wip:<15.4f} | {arabic_results.wip:<15.4f} |\")\n",
    "print(f\"| {'WIL':<15} | {english_results.wil:<15.4f} | {arabic_results.wil:<15.4f} |\")\n",
    "print(\"+\" + \"-\"*50 + \"+\")\n",
    "\n",
    "print(\"\\nğŸ‰ STT Evaluation Complete!\")\n",
    "print(\"âœ… All ADR-003 metrics calculated successfully\")\n",
    "print(\"âœ… Bilingual evaluation (English + Arabic) completed\")  \n",
    "print(\"âœ… Sample validation datasets processed\")\n",
    "print(\"âœ… Results exported for further analysis\")\n",
    "print(\"ğŸ“ Check the generated report file for detailed results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3104b",
   "metadata": {},
   "source": [
    "# STT Evaluation Results Summary\n",
    "\n",
    "## âœ… Successfully Implemented\n",
    "\n",
    "This notebook demonstrates the complete STT evaluation framework following ADR-003 specifications:\n",
    "\n",
    "### ğŸ¯ **Key Metrics Implemented**\n",
    "- **WER (Word Error Rate)**: Primary accuracy metric\n",
    "- **MER (Match Error Rate)**: Diagnostic complement to WER  \n",
    "- **WIP/WIL**: Information-theoretic perspectives\n",
    "- **CER (Character Error Rate)**: Orthographic accuracy\n",
    "- **Levenshtein Distances**: Edit distance measurements\n",
    "\n",
    "### ğŸŒ **Bilingual Support**\n",
    "- **English**: Standard normalization with punctuation handling\n",
    "- **Arabic**: Advanced normalization with diacritic stripping, Unicode NFKC\n",
    "\n",
    "### ğŸ“Š **Sample Results**\n",
    "- **English WER**: 0.0526 (Good performance, 5.26% error rate)\n",
    "- **Arabic WER**: 0.0357 (Excellent performance, 3.57% error rate)\n",
    "- **Domain Analysis**: Medical domain shows perfect accuracy\n",
    "- **Error Analysis**: Identifies challenging cases for improvement\n",
    "\n",
    "### ğŸ”§ **Data Structure**\n",
    "The evaluation uses structured JSON format with:\n",
    "```json\n",
    "{\n",
    "  \"id\": \"sample_identifier\",\n",
    "  \"reference\": \"ground_truth_transcription\", \n",
    "  \"hypothesis\": \"asr_system_output\",\n",
    "  \"metadata\": {\n",
    "    \"speaker_id\": \"speaker_info\",\n",
    "    \"domain\": \"content_type\",\n",
    "    \"dialect\": \"language_variant\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### ğŸš€ **Production Ready**\n",
    "- Comprehensive error handling\n",
    "- Standardized normalization\n",
    "- ADR-003 compliant metrics\n",
    "- Exportable results format\n",
    "- Bilingual evaluation support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2855bcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Installed Packages and Versions\n",
      "============================================================\n",
      "ğŸ” Using pip list:\n",
      "Package                                  Version\n",
      "---------------------------------------- ------------\n",
      "a2a-sdk                                  0.3.9\n",
      "agent-framework                          1.0.0b251007\n",
      "agent-framework-a2a                      1.0.0b251007\n",
      "agent-framework-azure-ai                 1.0.0b251007\n",
      "agent-framework-copilotstudio            1.0.0b251007\n",
      "agent-framework-core                     1.0.0b251007\n",
      "agent-framework-devui                    1.0.0b251007\n",
      "agent-framework-mem0                     1.0.0b251007\n",
      "agent-framework-redis                    1.0.0b251007\n",
      "aiofiles                                 25.1.0\n",
      "aiohappyeyeballs                         2.6.1\n",
      "aiohttp                                  3.13.0\n",
      "aiosignal                                1.4.0\n",
      "annotated-types                          0.7.0\n",
      "anyio                                    4.11.0\n",
      "asgiref                                  3.10.0\n",
      "asttokens                                3.0.0\n",
      "attrs                                    25.4.0\n",
      "autogen-agentchat                        0.7.5\n",
      "autogen-core                             0.7.5\n",
      "autogen-ext                              0.7.5\n",
      "azure-ai-agents                          1.2.0b5\n",
      "azure-ai-projects                        1.0.0\n",
      "azure-core                               1.36.0\n",
      "azure-core-tracing-opentelemetry         1.0.0b12\n",
      "azure-identity                           1.25.1\n",
      "azure-monitor-opentelemetry              1.8.1\n",
      "azure-monitor-opentelemetry-exporter     1.0.0b44\n",
      "azure-storage-blob                       12.27.0\n",
      "backoff                                  2.2.1\n",
      "cachetools                               6.2.1\n",
      "certifi                                  2025.10.5\n",
      "cffi                                     2.0.0\n",
      "charset-normalizer                       3.4.4\n",
      "click                                    8.3.0\n",
      "comm                                     0.2.3\n",
      "coverage                                 7.11.0\n",
      "cryptography                             46.0.3\n",
      "datasets                                 4.3.0\n",
      "debugpy                                  1.8.17\n",
      "decorator                                5.2.1\n",
      "dill                                     0.4.0\n",
      "distro                                   1.9.0\n",
      "dnspython                                2.8.0\n",
      "email-validator                          2.3.0\n",
      "executing                                2.2.1\n",
      "fastapi                                  0.119.0\n",
      "fastapi-cli                              0.0.13\n",
      "... and 159 more packages\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ Key STT Evaluation Packages:\n",
      "âœ… jiwer                4.0.0\n",
      "âœ… rapidfuzz            3.14.1\n",
      "âœ… pydantic             2.12.2\n",
      "âœ… numpy                2.3.4\n",
      "âœ… pandas               2.3.3\n",
      "âœ… datasets             4.3.0\n",
      "âœ… huggingface-hub      0.36.0\n",
      "âŒ transformers         Not installed\n",
      "âŒ torch                Not installed\n",
      "\n",
      "============================================================\n",
      "ğŸ Python Environment Info:\n",
      "Python version: 3.13.9 (main, Oct 17 2025, 05:47:12) [GCC 13.3.0]\n",
      "Python executable: /workspaces/oryx-cap-upskilling/.venv/bin/python\n",
      "Platform: linux\n",
      "Current directory: /workspaces/oryx-cap-upskilling/eval/test notebooks\n",
      "Python path length: 9 entries\n"
     ]
    }
   ],
   "source": [
    "# Show installed packages and their versions\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib.metadata\n",
    "\n",
    "print(\"ğŸ“¦ Installed Packages and Versions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Using pip list\n",
    "print(\"ğŸ” Using pip list:\")\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, '-m', 'pip', 'list'], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    # Show first 50 lines to avoid too much output\n",
    "    lines = result.stdout.split('\\n')\n",
    "    for line in lines[:50]:\n",
    "        if line.strip():\n",
    "            print(line)\n",
    "    if len(lines) > 50:\n",
    "        print(f\"... and {len(lines) - 50} more packages\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error running pip list: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Method 2: Show key packages for STT evaluation\n",
    "print(\"ğŸ¯ Key STT Evaluation Packages:\")\n",
    "key_packages = [\n",
    "    'jiwer', 'rapidfuzz', 'pydantic', 'numpy', 'pandas', \n",
    "    'datasets', 'huggingface-hub', 'transformers', 'torch'\n",
    "]\n",
    "\n",
    "for package in key_packages:\n",
    "    try:\n",
    "        version = importlib.metadata.version(package)\n",
    "        print(f\"âœ… {package:<20} {version}\")\n",
    "    except importlib.metadata.PackageNotFoundError:\n",
    "        print(f\"âŒ {package:<20} Not installed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Method 3: Show Python version and environment info\n",
    "print(\"ğŸ Python Environment Info:\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Platform: {sys.platform}\")\n",
    "\n",
    "# Show current working directory and path\n",
    "import os\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Python path length: {len(sys.path)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b6f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤ STT Evaluation Framework - Package Dependencies\n",
      "======================================================================\n",
      "ğŸ“‹ STT Framework Dependencies by Category:\n",
      "\n",
      "ğŸ”§ Speech Recognition Metrics:\n",
      "   âœ… jiwer                v4.0.0\n",
      "   âœ… rapidfuzz            v3.14.1\n",
      "\n",
      "ğŸ”§ Data Handling & Validation:\n",
      "   âœ… pydantic             v2.12.2\n",
      "   âœ… numpy                v2.3.4\n",
      "   âœ… pandas               v2.3.3\n",
      "\n",
      "ğŸ”§ Dataset Management:\n",
      "   âœ… datasets             v4.3.0\n",
      "   âœ… huggingface-hub      v0.36.0\n",
      "\n",
      "ğŸ”§ Deep Learning (Optional):\n",
      "   âŒ torch                Not installed\n",
      "   âŒ transformers         Not installed\n",
      "\n",
      "ğŸ”§ Development Tools:\n",
      "   âŒ jupyter              Not installed\n",
      "   âœ… ipython              v9.6.0\n",
      "   âœ… ruff                 v0.11.13\n",
      "\n",
      "ğŸ§ª STT Framework Module Tests:\n",
      "âœ… jiwer                     - Word Error Rate calculations\n",
      "âœ… rapidfuzz.distance        - Edit distance algorithms\n",
      "âœ… pydantic                  - Data validation\n",
      "âœ… numpy                     - Numerical operations\n",
      "\n",
      "ğŸ¯ Framework Status:\n",
      "âœ… STT Evaluation Framework: Successfully imported\n",
      "âœ… STT Evaluator: Instantiation successful\n",
      "âœ… Diarization Framework: Successfully imported\n",
      "\n",
      "ğŸ’¡ Installation Command for Missing Packages:\n",
      "pip install jiwer rapidfuzz pydantic numpy pandas datasets huggingface-hub\n"
     ]
    }
   ],
   "source": [
    "# Focus on STT Evaluation Framework Dependencies\n",
    "print(\"ğŸ¤ STT Evaluation Framework - Package Dependencies\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Core STT evaluation packages\n",
    "stt_packages = {\n",
    "    \"Speech Recognition Metrics\": [\n",
    "        'jiwer',           # Word error rate calculations\n",
    "        'rapidfuzz',       # Levenshtein distance computations\n",
    "    ],\n",
    "    \"Data Handling & Validation\": [\n",
    "        'pydantic',        # Data validation and settings\n",
    "        'numpy',           # Numerical computations  \n",
    "        'pandas',          # Data manipulation\n",
    "    ],\n",
    "    \"Dataset Management\": [\n",
    "        'datasets',        # HuggingFace datasets\n",
    "        'huggingface-hub', # Dataset downloading\n",
    "    ],\n",
    "    \"Deep Learning (Optional)\": [\n",
    "        'torch',           # PyTorch for advanced models\n",
    "        'transformers',    # HuggingFace transformers\n",
    "    ],\n",
    "    \"Development Tools\": [\n",
    "        'jupyter',         # Notebook environment\n",
    "        'ipython',         # Interactive Python\n",
    "        'ruff',            # Code linting (may not be in pip list)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ STT Framework Dependencies by Category:\")\n",
    "print()\n",
    "\n",
    "for category, packages in stt_packages.items():\n",
    "    print(f\"ğŸ”§ {category}:\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            version = importlib.metadata.version(package)\n",
    "            print(f\"   âœ… {package:<20} v{version}\")\n",
    "        except importlib.metadata.PackageNotFoundError:\n",
    "            print(f\"   âŒ {package:<20} Not installed\")\n",
    "    print()\n",
    "\n",
    "# Check STT evaluation specific imports\n",
    "print(\"ğŸ§ª STT Framework Module Tests:\")\n",
    "stt_modules = [\n",
    "    ('jiwer', 'Word Error Rate calculations'),\n",
    "    ('rapidfuzz.distance', 'Edit distance algorithms'),\n",
    "    ('pydantic', 'Data validation'),\n",
    "    ('numpy', 'Numerical operations'),\n",
    "]\n",
    "\n",
    "for module, description in stt_modules:\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"âœ… {module:<25} - {description}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ {module:<25} - Failed: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ¯ Framework Status:\")\n",
    "try:\n",
    "    from eval.stt.main import SpeechToTextEvaluator\n",
    "    print(\"âœ… STT Evaluation Framework: Successfully imported\")\n",
    "    evaluator = SpeechToTextEvaluator()\n",
    "    print(\"âœ… STT Evaluator: Instantiation successful\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ STT Framework: Import failed - {e}\")\n",
    "\n",
    "try:\n",
    "    from eval.diarization.main import DiarizationEvaluator\n",
    "    print(\"âœ… Diarization Framework: Successfully imported\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Diarization Framework: Import failed - {e}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ’¡ Installation Command for Missing Packages:\")\n",
    "print(\"pip install jiwer rapidfuzz pydantic numpy pandas datasets huggingface-hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c424724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Azure ML Endpoint Configuration\n",
    "# Request data goes here - The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "\n",
    "# Sample data payload - replace with your actual data\n",
    "data = {\n",
    "    \"text\": \"Sample text for evaluation\",\n",
    "    \"language\": \"en\"\n",
    "}\n",
    "\n",
    "# Encode the data as JSON\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "# Azure ML endpoint URL\n",
    "url = 'https://ml-allam.eastus2.inference.ml.azure.com/score'\n",
    "\n",
    "# Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "# You should set this from environment variables or secure configuration\n",
    "api_key = ''  # TODO: Set your API key here\n",
    "\n",
    "if not api_key:\n",
    "    print(\"âš ï¸  Warning: No API key provided. Please set your API key to test the endpoint.\")\n",
    "    print(\"You can get the API key from Azure ML Studio > Endpoints > Your Endpoint > Consume\")\n",
    "else:\n",
    "    # Set up headers for the request\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json',\n",
    "        'Authorization': ('Bearer ' + api_key)\n",
    "    }\n",
    "    \n",
    "    # Create the request\n",
    "    req = urllib.request.Request(url, body, headers)\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ”„ Making request to: {url}\")\n",
    "        print(f\"ğŸ“¤ Payload: {json.dumps(data, indent=2)}\")\n",
    "        \n",
    "        response = urllib.request.urlopen(req)\n",
    "        result = response.read()\n",
    "        \n",
    "        print(\"âœ… Request successful!\")\n",
    "        print(f\"ğŸ“¥ Response: {result.decode('utf-8')}\")\n",
    "        \n",
    "        # Try to parse as JSON for better formatting\n",
    "        try:\n",
    "            result_json = json.loads(result.decode('utf-8'))\n",
    "            print(f\"ğŸ“Š Formatted Response: {json.dumps(result_json, indent=2)}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"ğŸ“„ Raw Response: {result.decode('utf-8')}\")\n",
    "            \n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(f\"âŒ Request failed with status code: {error.code}\")\n",
    "        print(f\"ğŸ“‹ Error info: {error.info()}\")\n",
    "        \n",
    "        # Print the error response body for debugging\n",
    "        error_body = error.read().decode(\"utf8\", 'ignore')\n",
    "        print(f\"ğŸ” Error details: {error_body}\")\n",
    "        \n",
    "        # Try to parse error as JSON\n",
    "        try:\n",
    "            error_json = json.loads(error_body)\n",
    "            print(f\"ğŸ“Š Formatted Error: {json.dumps(error_json, indent=2)}\")\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Unexpected error: {e}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Next steps:\")\n",
    "print(\"1. Set your API key in the 'api_key' variable\")\n",
    "print(\"2. Update the 'data' payload with your actual input\")\n",
    "print(\"3. Run the cell to test the endpoint\")\n",
    "print(\"4. Check Azure ML Studio for endpoint logs if needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-upskilling-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
